"""Main entry point for the Evidence-Grounded Backstory Consistency System."""
import logging
import os
from datetime import datetime
from pathlib import Path
from typing import TypedDict
from collections import defaultdict

import pandas as pd
from dotenv import load_dotenv
from langgraph.graph import StateGraph, END

from extraction_agent.main import extract
from graph_creator_agent.main import create_graph
from answering_agent.main import answer
from Graphrag.pathway.build_index import build_index
from Graphrag.pathway.retriever import retrieve_topk
from input import get_input_data
from shared_config import ROW_DELAY_SECONDS
import time

# Load environment variables
load_dotenv()

class PipelineState(TypedDict):
    """Global state for the LangGraph pipeline."""
    book_name: str
    character_name: str
    backstory: str
    
    queries: list[str]
    evidences: list[dict]  # {id, text}
    
    graph_path: str | None
    
    label: int | None
    reasoning: str | None
    evidence_queries: list[str] | None  # Queries generated by classifier
    evidence_chunks: list[dict] | None  # Chunks retrieved for queries
    
    nli_avg_entailment: float | None
    nli_max_contradiction: float | None
    nli_avg_contradiction: float | None


def setup_logging() -> str:
    """Set up logging with file and console handlers.
    
    Returns:
        Path to the log file created.
    """
    logs_dir = Path("logs")
    logs_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = logs_dir / f"run_{timestamp}.log"
    
    # Create formatter with machine-readable format
    formatter = logging.Formatter(
        fmt='%(asctime)s | %(levelname)-8s | %(name)s | %(funcName)s:%(lineno)d | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # File handler
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    
    # Root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)
    root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)
    
    logging.info(f"Logging initialized. Log file: {log_file}")
    return str(log_file)




def print_output(state: PipelineState):
    """Print formatted output."""
    label_text = "CONSISTENT" if state["label"] == 1 else "CONTRADICTING"
    
    print(f"\nLabel: {label_text} ({state['label']})")
    
    # Print NLI Metrics (Bold with spaces as requested)
    avg_ent = state.get("nli_avg_entailment", 0.0)
    max_con = state.get("nli_max_contradiction", 0.0)
    avg_con = state.get("nli_avg_contradiction", 0.0)
    
    print(f"\n{'='*60}")
    print(f"\033[1m  NLI CONSISTENCY VERIFICATION  \033[0m")
    print(f"{'='*60}")
    print(f"\n\033[1m  MAX CONTRADICTION SCORE             : {max_con:.4f}  \033[0m")
    print(f"\n\033[1m  AVERAGE ENTAILMENT (CONSISTENCY)    : {avg_ent:.4f}  \033[0m")
    print(f"\n\033[1m  AVERAGE CONTRADICTION SCORE         : {avg_con:.4f}  \033[0m")
    print(f"\n{'='*60}\n")

    print(f"\nReasoning:\n{state['reasoning']}\n")
    
    # Print evidence queries
    print("Evidence Queries Generated:")
    if state.get("evidence_queries"):
        for idx, query in enumerate(state["evidence_queries"], 1):
            print(f"  {idx}. {query}")
    else:
        print("  - No queries generated")
    
    print("\nSupporting Evidence from Novel:")
    if state.get("evidence_chunks"):
        for chunk in state["evidence_chunks"]:
            chunk_id = chunk.get("id", "unknown")
            chunk_text = chunk.get("text", "N/A")
            query = chunk.get("query", "N/A")
            score = chunk.get("score", 0)
            print(f"\n  [{chunk_id}] (score: {score:.3f})")
            print(f"  Query: '{query}'")
            print(f"  Text: {chunk_text[:300]}...")  # Show first 300 chars
    else:
        print("  - No evidence retrieved")


def run_pipeline_for_row(row_data: dict) -> PipelineState:
    """Run the pipeline for a single row of data.
    
    Args:
        row_data: Dictionary with book_name, char, and content.
        
    Returns:
        Final pipeline state.
    """
    logger = logging.getLogger(__name__)
    
    row_data["evidences"] = retrieve_topk(row_data["book_name"], row_data["char"])
    # Initialize state
    initial_state: PipelineState = {
        "book_name": row_data["book_name"],
        "character_name": row_data["char"],
        "backstory": row_data["content"],
        "queries": [],
        "evidences": row_data["evidences"],
        "graph_path": None,
        "label": None,
        "reasoning": None,
        "evidence_queries": None,
        "evidence_chunks": None,
        "nli_avg_entailment": None,
        "nli_max_contradiction": None,
        "nli_avg_contradiction": None,
    }
    
    logger.info(f"Processing: {row_data['char']} from {row_data['book_name']}")
    
    # Create LangGraph workflow
    workflow = StateGraph(PipelineState)
    
    # Add nodes
    workflow.add_node("extract", extract)
    workflow.add_node("create_graph", create_graph)
    workflow.add_node("answer", answer)
    
    # Add edges
    workflow.set_entry_point("extract")
    workflow.add_edge("extract", "create_graph")
    workflow.add_edge("create_graph", "answer")
    workflow.add_edge("answer", END)
    
    # Compile graph
    app = workflow.compile()
    
    logger.info("Running pipeline")
    
    # Run pipeline
    final_state = app.invoke(initial_state)
    logger.info("Pipeline completed successfully")
    
    return final_state


def main():
    """Main entry point."""
    # Setup logging
    log_file = setup_logging()
    logger = logging.getLogger(__name__)
    logger.info("Starting Evidence-Grounded Backstory Consistency System")

    # Get row indices to process from user input
    try:
        input_str = input("Enter space-separated row indices (0-indexed, e.g., '0 5 7'): ")
        if not input_str.strip():
            # Default to 0 if empty
            row_indices = [0]
            logger.info("No input provided, defaulting to row 0")
        else:
            row_indices = [int(x) for x in input_str.split()]
            
    except ValueError:
        row_indices = [0]
        logger.info("Invalid input, using default: row 0")

    # Read data using input.py function
    try:
        df = get_input_data(row_indices)
        logger.info(f"Loaded {len(df)} rows from train.csv: {row_indices}")
        logger.info(f"Processing {len(df)} backstories")
    except Exception as e:
        logger.error(f"Error reading train.csv: {e}")
        raise
    
    # Process each row
    results = []
    output_file = "output.csv"
    # Initialize output file with headers
    pd.DataFrame(columns=["book_name", "character_name", "predictions", "reasoning"]).to_csv(output_file, index=False)
    logger.info(f"Initialized {output_file}")

    flag = defaultdict(int)
    for i, (idx, row) in enumerate(df.iterrows()):
        print(f"\n{'='*80}")
        print(f"[{i + 1}/{len(df)}] Processing CSV Row Index: {idx}")
        print(f"Character: {row['char']}")
        print(f"Book: {row['book_name']}")
        print(f"{'='*80}\n")
        # topchunks = retrieve_topk("")

        book_path = ""
        # Handle book name matching (CSV has "In Search of the Castaways" with title case)
        book_name_lower = row['book_name'].lower()
        if book_name_lower == "in search of the castaways":
            book_path = "Books/In search of the castaways.txt"
        elif row['book_name'] == "The Count of Monte Cristo":
            book_path = "Books/The Count of Monte Cristo.txt"
        else:
            logger.warning(f"Book '{row['book_name']}' not found in predefined paths.")
            print(f"❌ Book '{row['book_name']}' not found. Skipping this row.")
            continue

        if flag[row['book_name']] == 0:
            # build_index will normalize book name to lowercase internally
            # Pass defaults for optional parameters (will use env vars or defaults)
            build_index(book_path, row['book_name'])
            flag[row['book_name']] = 1

        try:
            row_data = {
                "book_name": row["book_name"],
                "char": row["char"],
                "content": row["content"]
            }
            
            final_state = run_pipeline_for_row(row_data)
            results.append(final_state)
            
            # Print output for this row
            print_output(final_state)

            # SAVE INCREMENTALLY
            label_str = "CONSISTENT" if final_state.get("label") == 1 else "CONTRADICTING"
            reasoning = final_state.get("reasoning", "")
            current_row_df = pd.DataFrame([{
                "book_name": final_state.get("book_name"),
                "character_name": final_state.get("character_name"),
                "predictions": label_str,
                "reasoning": reasoning
            }])
            current_row_df.to_csv(output_file, mode='a', header=False, index=False)
            logger.info(f"Appended result for {row['char']} to {output_file}")
            
        except Exception as e:
            logger.error(f"Error processing row index {idx}: {e}", exc_info=True)
            print(f"❌ Error processing this row: {e}")
            continue
        
        # Delay before next row
        if i < len(df) - 1:  # Don't sleep after the last row
            logger.info(f"Waiting {ROW_DELAY_SECONDS} seconds before next row...")
            print(f"Waiting {ROW_DELAY_SECONDS} seconds...")
            time.sleep(ROW_DELAY_SECONDS)
    
    print(f"\n{'='*80}")
    print(f"Completed processing {len(results)}/{len(df)} backstories")
    print(f"{'='*80}\n")

    # Save output to CSV
    if results:
        output_rows = []
        for state in results:
            label_str = "CONSISTENT" if state.get("label") == 1 else "CONTRADICTING"
            reasoning = state.get("reasoning", "")
            output_rows.append({
                "book_name": state.get("book_name"),
                "character_name": state.get("character_name"),
                "predictions": label_str,
                "reasoning": reasoning
            })
        
        output_df = pd.DataFrame(output_rows)
        output_file = "output.csv"
        output_df.to_csv(output_file, index=False)
        logger.info(f"Saved results to {output_file}")
        print(f"✅ Results saved to {output_file}")
    else:
        logger.warning("No results to save.")



if __name__ == "__main__":
    main()