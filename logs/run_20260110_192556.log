2026-01-10 19:25:56 | INFO     | root | setup_logging:72 | Logging initialized. Log file: logs/run_20260110_192556.log
2026-01-10 19:25:56 | INFO     | __main__ | main:153 | Starting Evidence-Grounded Backstory Consistency System
2026-01-10 19:25:57 | INFO     | __main__ | main:165 | Loaded 5 rows from train.csv
2026-01-10 19:25:57 | INFO     | __main__ | main:166 | Processing 5 backstories
2026-01-10 19:25:57 | INFO     | __main__ | main:176 | Initialized output.csv
2026-01-10 19:25:58 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 19:25:58 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12573a690>
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'GET']>
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'GET']>
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'GET']>
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-type', b'application/json'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-encoding', b'zstd'), (b'date', b'Sat, 10 Jan 2026 13:55:57 GMT')])
2026-01-10 19:25:58 | INFO     | httpx | _send_single_request:1025 | HTTP Request: GET http://localhost:6333 "HTTP/1.1 200 OK"
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'GET']>
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 19:25:58 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 19:25:58 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 19:25:58 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 19:25:58 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12573b200>
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'GET']>
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'GET']>
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'GET']>
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-encoding', b'zstd'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-type', b'application/json'), (b'date', b'Sat, 10 Jan 2026 13:55:57 GMT')])
2026-01-10 19:25:58 | INFO     | httpx | _send_single_request:1025 | HTTP Request: GET http://localhost:6333/collections "HTTP/1.1 200 OK"
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'GET']>
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 19:25:58 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 19:25:58 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 19:25:58 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 19:25:58 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12573bc20>
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'GET']>
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'GET']>
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'GET']>
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-type', b'application/json'), (b'content-encoding', b'zstd'), (b'date', b'Sat, 10 Jan 2026 13:55:57 GMT')])
2026-01-10 19:25:58 | INFO     | httpx | _send_single_request:1025 | HTTP Request: GET http://localhost:6333/collections/in%20search%20of%20the%20castaways_collection "HTTP/1.1 200 OK"
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'GET']>
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 19:25:58 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 19:25:58 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 19:25:58 | INFO     | __main__ | run_pipeline_for_row:120 | Processing: Thalcave from In Search of the Castaways
2026-01-10 19:25:58 | INFO     | __main__ | run_pipeline_for_row:139 | Running pipeline
2026-01-10 19:25:58 | INFO     | extraction_agent.main | extract:42 | Starting extraction agent
2026-01-10 19:25:58 | INFO     | extraction_agent.main | extract:43 | Book: In Search of the Castaways, Character: Thalcave
2026-01-10 19:25:58 | DEBUG    | extraction_agent.main | extract:56 | Extraction prompt: You are an extractor agent.
Generate 5 queries to retrieve evidence about the character.

Book: In Search of the Castaways
Character: Thalcave
Backstory: Thalcave’s people faded as colonists advanced; his father, last of the tribal guides, knew the pampas geography and animal ways, while his mother died giving birth. Boyhood was spent roaming the plains with his father, learning to track, tame horses and steer by the stars.

Return only a JSON list of strings, each string being a query.
Example: ["query 1", "query 2", "query 3"]

2026-01-10 19:25:58 | INFO     | extraction_agent.main | extract:59 | Generating queries via LLM
2026-01-10 19:25:58 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-37b202c7-2a35-4155-81e0-6f113c241707', 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: In Search of the Castaways\nCharacter: Thalcave\nBackstory: Thalcave’s people faded as colonists advanced; his father, last of the tribal guides, knew the pampas geography and animal ways, while his mother died giving birth. Boyhood was spent roaming the plains with his father, learning to track, tame horses and steer by the stars.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'stream': False}}
2026-01-10 19:25:58 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 19:25:58 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=None socket_options=None
2026-01-10 19:25:58 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1265f0bf0>
2026-01-10 19:25:58 | DEBUG    | httpcore.connection | trace:47 | start_tls.started ssl_context=<ssl.SSLContext object at 0x12607af50> server_hostname='openrouter.ai' timeout=None
2026-01-10 19:25:58 | DEBUG    | httpcore.connection | trace:47 | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1261546e0>
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 13:55:58 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbcacd1f8bac0e2-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 19:25:58 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 19:25:58 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 19:25:58 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 13:55:58 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbcacd1f8bac0e2-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 19:25:58 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 19:25:58 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 19:25:58 | DEBUG    | openai._base_client | _should_retry:774 | Retrying due to status code 429
2026-01-10 19:25:58 | DEBUG    | openai._base_client | _sleep_for_retry:1068 | 2 retries left
2026-01-10 19:25:58 | INFO     | openai._base_client | _sleep_for_retry:1071 | Retrying request to /chat/completions in 0.409701 seconds
2026-01-10 19:25:59 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-37b202c7-2a35-4155-81e0-6f113c241707', 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: In Search of the Castaways\nCharacter: Thalcave\nBackstory: Thalcave’s people faded as colonists advanced; his father, last of the tribal guides, knew the pampas geography and animal ways, while his mother died giving birth. Boyhood was spent roaming the plains with his father, learning to track, tame horses and steer by the stars.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'stream': False}}
2026-01-10 19:25:59 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 19:25:59 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 19:25:59 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 19:25:59 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 19:25:59 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 19:25:59 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 19:25:59 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 13:55:59 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbcacd81e48c0e2-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 19:25:59 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 19:25:59 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 19:25:59 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 19:25:59 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 19:25:59 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 19:25:59 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 13:55:59 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbcacd81e48c0e2-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 19:25:59 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 19:25:59 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 19:25:59 | DEBUG    | openai._base_client | _should_retry:774 | Retrying due to status code 429
2026-01-10 19:25:59 | DEBUG    | openai._base_client | _sleep_for_retry:1066 | 1 retry left
2026-01-10 19:25:59 | INFO     | openai._base_client | _sleep_for_retry:1071 | Retrying request to /chat/completions in 0.794834 seconds
2026-01-10 19:26:00 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-37b202c7-2a35-4155-81e0-6f113c241707', 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: In Search of the Castaways\nCharacter: Thalcave\nBackstory: Thalcave’s people faded as colonists advanced; his father, last of the tribal guides, knew the pampas geography and animal ways, while his mother died giving birth. Boyhood was spent roaming the plains with his father, learning to track, tame horses and steer by the stars.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'stream': False}}
2026-01-10 19:26:00 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 19:26:00 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 19:26:00 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 19:26:00 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 19:26:00 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 19:26:00 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 19:26:00 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 13:56:01 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbcace0cdc4c0e2-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 19:26:00 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 19:26:00 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 19:26:00 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 19:26:00 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 19:26:00 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 19:26:00 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 13:56:01 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbcace0cdc4c0e2-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 19:26:00 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 19:26:00 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 19:26:00 | DEBUG    | openai._base_client | request:1046 | Re-raising status error
2026-01-10 19:26:00 | ERROR    | __main__ | main:231 | Error processing row 1: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1768089600000'}, 'provider_name': None}}, 'user_id': 'user_2yUa3CBBFo0pOw0zUiHXS4PLFt0'}
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/main.py", line 212, in main
    final_state = run_pipeline_for_row(row_data)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/main.py", line 142, in run_pipeline_for_row
    final_state = app.invoke(initial_state)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py", line 3068, in invoke
    for chunk in self.stream(
                 ^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py", line 2643, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py", line 167, in tick
    run_with_retry(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py", line 656, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py", line 400, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/extraction_agent/main.py", line 60, in extract
    response = llm.invoke(prompt)
               ^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 402, in invoke
    self.generate_prompt(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 931, in generate
    self._generate_with_cache(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py", line 1386, in _generate
    raise e
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py", line 1381, in _generate
    raw_response = self.client.with_raw_response.create(**payload)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1768089600000'}, 'provider_name': None}}, 'user_id': 'user_2yUa3CBBFo0pOw0zUiHXS4PLFt0'}
During task with name 'extract' and id 'ef927c1c-6dff-175f-a447-153b0ce2178e'
2026-01-10 19:26:01 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 19:26:01 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1261dfe90>
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'GET']>
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'GET']>
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'GET']>
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-type', b'application/json'), (b'content-encoding', b'zstd'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'date', b'Sat, 10 Jan 2026 13:56:00 GMT')])
2026-01-10 19:26:01 | INFO     | httpx | _send_single_request:1025 | HTTP Request: GET http://localhost:6333 "HTTP/1.1 200 OK"
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'GET']>
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 19:26:01 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 19:26:01 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 19:26:01 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 19:26:01 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1266809b0>
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'GET']>
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'GET']>
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'GET']>
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-encoding', b'zstd'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-type', b'application/json'), (b'date', b'Sat, 10 Jan 2026 13:56:00 GMT')])
2026-01-10 19:26:01 | INFO     | httpx | _send_single_request:1025 | HTTP Request: GET http://localhost:6333/collections "HTTP/1.1 200 OK"
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'GET']>
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 19:26:01 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 19:26:01 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 19:26:01 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 19:26:01 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12676d940>
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'GET']>
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'GET']>
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'GET']>
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-type', b'application/json'), (b'content-encoding', b'zstd'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'date', b'Sat, 10 Jan 2026 13:56:00 GMT')])
2026-01-10 19:26:01 | INFO     | httpx | _send_single_request:1025 | HTTP Request: GET http://localhost:6333/collections/the%20count%20of%20monte%20cristo_collection "HTTP/1.1 200 OK"
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'GET']>
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 19:26:01 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 19:26:01 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 19:26:01 | INFO     | __main__ | run_pipeline_for_row:120 | Processing: Faria from The Count of Monte Cristo
2026-01-10 19:26:01 | INFO     | __main__ | run_pipeline_for_row:139 | Running pipeline
2026-01-10 19:26:01 | INFO     | extraction_agent.main | extract:42 | Starting extraction agent
2026-01-10 19:26:01 | INFO     | extraction_agent.main | extract:43 | Book: The Count of Monte Cristo, Character: Faria
2026-01-10 19:26:01 | DEBUG    | extraction_agent.main | extract:56 | Extraction prompt: You are an extractor agent.
Generate 5 queries to retrieve evidence about the character.

Book: The Count of Monte Cristo
Character: Faria
Backstory: Suspected again in 1815, he was re-arrested and shipped to the Château d’If, this time for life.

Return only a JSON list of strings, each string being a query.
Example: ["query 1", "query 2", "query 3"]

2026-01-10 19:26:01 | INFO     | extraction_agent.main | extract:59 | Generating queries via LLM
2026-01-10 19:26:01 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-90c9b735-1898-465a-9f92-036d58a17f47', 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: The Count of Monte Cristo\nCharacter: Faria\nBackstory: Suspected again in 1815, he was re-arrested and shipped to the Château d’If, this time for life.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'stream': False}}
2026-01-10 19:26:01 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 13:56:01 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbcace36b97c0e2-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 19:26:01 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 19:26:01 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 13:56:01 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbcace36b97c0e2-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 19:26:01 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 19:26:01 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 19:26:01 | DEBUG    | openai._base_client | _should_retry:774 | Retrying due to status code 429
2026-01-10 19:26:01 | DEBUG    | openai._base_client | _sleep_for_retry:1068 | 2 retries left
2026-01-10 19:26:01 | INFO     | openai._base_client | _sleep_for_retry:1071 | Retrying request to /chat/completions in 0.388052 seconds
2026-01-10 19:26:01 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-90c9b735-1898-465a-9f92-036d58a17f47', 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: The Count of Monte Cristo\nCharacter: Faria\nBackstory: Suspected again in 1815, he was re-arrested and shipped to the Château d’If, this time for life.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'stream': False}}
2026-01-10 19:26:01 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 19:26:01 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 19:26:02 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 13:56:02 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbcace83e4ec0e2-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 19:26:02 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 19:26:02 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 19:26:02 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 19:26:02 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 19:26:02 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 19:26:02 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 13:56:02 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbcace83e4ec0e2-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 19:26:02 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 19:26:02 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 19:26:02 | DEBUG    | openai._base_client | _should_retry:774 | Retrying due to status code 429
2026-01-10 19:26:02 | DEBUG    | openai._base_client | _sleep_for_retry:1066 | 1 retry left
2026-01-10 19:26:02 | INFO     | openai._base_client | _sleep_for_retry:1071 | Retrying request to /chat/completions in 0.763263 seconds
2026-01-10 19:26:02 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-90c9b735-1898-465a-9f92-036d58a17f47', 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: The Count of Monte Cristo\nCharacter: Faria\nBackstory: Suspected again in 1815, he was re-arrested and shipped to the Château d’If, this time for life.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'stream': False}}
2026-01-10 19:26:02 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 19:26:02 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 19:26:02 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 19:26:02 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 19:26:02 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 19:26:02 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 19:26:03 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 13:56:03 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbcaceeed47c0e2-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 19:26:03 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 19:26:03 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 19:26:03 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 19:26:03 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 19:26:03 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 19:26:03 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 13:56:03 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbcaceeed47c0e2-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 19:26:03 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 19:26:03 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 19:26:03 | DEBUG    | openai._base_client | request:1046 | Re-raising status error
2026-01-10 19:26:03 | ERROR    | __main__ | main:231 | Error processing row 2: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1768089600000'}, 'provider_name': None}}, 'user_id': 'user_2yUa3CBBFo0pOw0zUiHXS4PLFt0'}
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/main.py", line 212, in main
    final_state = run_pipeline_for_row(row_data)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/main.py", line 142, in run_pipeline_for_row
    final_state = app.invoke(initial_state)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py", line 3068, in invoke
    for chunk in self.stream(
                 ^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py", line 2643, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py", line 167, in tick
    run_with_retry(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py", line 656, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py", line 400, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/extraction_agent/main.py", line 60, in extract
    response = llm.invoke(prompt)
               ^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 402, in invoke
    self.generate_prompt(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 931, in generate
    self._generate_with_cache(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py", line 1386, in _generate
    raise e
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py", line 1381, in _generate
    raw_response = self.client.with_raw_response.create(**payload)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1768089600000'}, 'provider_name': None}}, 'user_id': 'user_2yUa3CBBFo0pOw0zUiHXS4PLFt0'}
During task with name 'extract' and id 'dd8d8839-f366-c083-f418-77be5872efc5'
2026-01-10 19:26:03 | INFO     | __main__ | run_pipeline_for_row:120 | Processing: Kai-Koumou from In Search of the Castaways
2026-01-10 19:26:03 | INFO     | __main__ | run_pipeline_for_row:139 | Running pipeline
2026-01-10 19:26:03 | INFO     | extraction_agent.main | extract:42 | Starting extraction agent
2026-01-10 19:26:03 | INFO     | extraction_agent.main | extract:43 | Book: In Search of the Castaways, Character: Kai-Koumou
2026-01-10 19:26:03 | DEBUG    | extraction_agent.main | extract:56 | Extraction prompt: You are an extractor agent.
Generate 5 queries to retrieve evidence about the character.

Book: In Search of the Castaways
Character: Kai-Koumou
Backstory: Before each fight he studied the crack-patterns of his mother’s shark-tooth necklace by moonlight to divine the outcome.

Return only a JSON list of strings, each string being a query.
Example: ["query 1", "query 2", "query 3"]

2026-01-10 19:26:03 | INFO     | extraction_agent.main | extract:59 | Generating queries via LLM
2026-01-10 19:26:03 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-3fcc35f7-2bea-4e30-b444-2461154d309c', 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: In Search of the Castaways\nCharacter: Kai-Koumou\nBackstory: Before each fight he studied the crack-patterns of his mother’s shark-tooth necklace by moonlight to divine the outcome.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'stream': False}}
2026-01-10 19:26:03 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 19:26:03 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 19:26:03 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 19:26:03 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 19:26:03 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 19:26:03 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 19:26:03 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 13:56:03 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbcacf16af2c0e2-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 19:26:03 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 19:26:03 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 19:26:03 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 19:26:03 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 19:26:03 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 19:26:03 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 13:56:03 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbcacf16af2c0e2-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 19:26:03 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 19:26:03 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 19:26:03 | DEBUG    | openai._base_client | _should_retry:774 | Retrying due to status code 429
2026-01-10 19:26:03 | DEBUG    | openai._base_client | _sleep_for_retry:1068 | 2 retries left
2026-01-10 19:26:03 | INFO     | openai._base_client | _sleep_for_retry:1071 | Retrying request to /chat/completions in 0.427154 seconds
2026-01-10 19:26:04 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-3fcc35f7-2bea-4e30-b444-2461154d309c', 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: In Search of the Castaways\nCharacter: Kai-Koumou\nBackstory: Before each fight he studied the crack-patterns of his mother’s shark-tooth necklace by moonlight to divine the outcome.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'stream': False}}
2026-01-10 19:26:04 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 19:26:04 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 19:26:04 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 19:26:04 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 19:26:04 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 19:26:04 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 19:26:04 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 13:56:04 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbcacf68ea7c0e2-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 19:26:04 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 19:26:04 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 19:26:04 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 19:26:04 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 19:26:04 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 19:26:04 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 13:56:04 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbcacf68ea7c0e2-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 19:26:04 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 19:26:04 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 19:26:04 | DEBUG    | openai._base_client | _should_retry:774 | Retrying due to status code 429
2026-01-10 19:26:04 | DEBUG    | openai._base_client | _sleep_for_retry:1066 | 1 retry left
2026-01-10 19:26:04 | INFO     | openai._base_client | _sleep_for_retry:1071 | Retrying request to /chat/completions in 0.993378 seconds
2026-01-10 19:26:05 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-3fcc35f7-2bea-4e30-b444-2461154d309c', 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: In Search of the Castaways\nCharacter: Kai-Koumou\nBackstory: Before each fight he studied the crack-patterns of his mother’s shark-tooth necklace by moonlight to divine the outcome.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'stream': False}}
2026-01-10 19:26:05 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 19:26:05 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 19:26:05 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 19:26:05 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 19:26:05 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 19:26:05 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 19:26:05 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 13:56:05 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbcacfeb9c6c0e2-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 19:26:05 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 19:26:05 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 19:26:05 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 19:26:05 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 19:26:05 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 19:26:05 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 13:56:05 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbcacfeb9c6c0e2-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 19:26:05 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 19:26:05 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 19:26:05 | DEBUG    | openai._base_client | request:1046 | Re-raising status error
2026-01-10 19:26:05 | ERROR    | __main__ | main:231 | Error processing row 3: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1768089600000'}, 'provider_name': None}}, 'user_id': 'user_2yUa3CBBFo0pOw0zUiHXS4PLFt0'}
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/main.py", line 212, in main
    final_state = run_pipeline_for_row(row_data)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/main.py", line 142, in run_pipeline_for_row
    final_state = app.invoke(initial_state)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py", line 3068, in invoke
    for chunk in self.stream(
                 ^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py", line 2643, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py", line 167, in tick
    run_with_retry(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py", line 656, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py", line 400, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/extraction_agent/main.py", line 60, in extract
    response = llm.invoke(prompt)
               ^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 402, in invoke
    self.generate_prompt(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 931, in generate
    self._generate_with_cache(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py", line 1386, in _generate
    raise e
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py", line 1381, in _generate
    raw_response = self.client.with_raw_response.create(**payload)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1768089600000'}, 'provider_name': None}}, 'user_id': 'user_2yUa3CBBFo0pOw0zUiHXS4PLFt0'}
During task with name 'extract' and id 'dd43a849-0451-3bb6-42c9-adb915643e42'
2026-01-10 19:26:05 | INFO     | __main__ | run_pipeline_for_row:120 | Processing: Noirtier from The Count of Monte Cristo
2026-01-10 19:26:05 | INFO     | __main__ | run_pipeline_for_row:139 | Running pipeline
2026-01-10 19:26:05 | INFO     | extraction_agent.main | extract:42 | Starting extraction agent
2026-01-10 19:26:05 | INFO     | extraction_agent.main | extract:43 | Book: The Count of Monte Cristo, Character: Noirtier
2026-01-10 19:26:05 | DEBUG    | extraction_agent.main | extract:56 | Extraction prompt: You are an extractor agent.
Generate 5 queries to retrieve evidence about the character.

Book: The Count of Monte Cristo
Character: Noirtier
Backstory: Villefort’s drift toward the royalists disappointed him; father and son argued politics at every family gathering.

Return only a JSON list of strings, each string being a query.
Example: ["query 1", "query 2", "query 3"]

2026-01-10 19:26:05 | INFO     | extraction_agent.main | extract:59 | Generating queries via LLM
2026-01-10 19:26:05 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-4901e232-01b3-44b0-a667-c4a716a65c73', 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: The Count of Monte Cristo\nCharacter: Noirtier\nBackstory: Villefort’s drift toward the royalists disappointed him; father and son argued politics at every family gathering.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'stream': False}}
2026-01-10 19:26:05 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 19:26:05 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 19:26:05 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 19:26:05 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 19:26:05 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 19:26:05 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 19:26:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 13:56:06 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbcad00ee46c0e2-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 19:26:06 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 19:26:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 19:26:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 19:26:06 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 19:26:06 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 19:26:06 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 13:56:06 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbcad00ee46c0e2-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 19:26:06 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 19:26:06 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 19:26:06 | DEBUG    | openai._base_client | _should_retry:774 | Retrying due to status code 429
2026-01-10 19:26:06 | DEBUG    | openai._base_client | _sleep_for_retry:1068 | 2 retries left
2026-01-10 19:26:06 | INFO     | openai._base_client | _sleep_for_retry:1071 | Retrying request to /chat/completions in 0.491311 seconds
2026-01-10 19:26:06 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-4901e232-01b3-44b0-a667-c4a716a65c73', 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: The Count of Monte Cristo\nCharacter: Noirtier\nBackstory: Villefort’s drift toward the royalists disappointed him; father and son argued politics at every family gathering.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'stream': False}}
2026-01-10 19:26:06 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 19:26:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 19:26:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 19:26:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 19:26:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 19:26:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 19:26:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 13:56:06 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbcad05fa6dc0e2-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 19:26:06 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 19:26:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 19:26:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 19:26:06 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 19:26:06 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 19:26:06 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 13:56:06 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbcad05fa6dc0e2-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 19:26:06 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 19:26:06 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 19:26:06 | DEBUG    | openai._base_client | _should_retry:774 | Retrying due to status code 429
2026-01-10 19:26:06 | DEBUG    | openai._base_client | _sleep_for_retry:1066 | 1 retry left
2026-01-10 19:26:06 | INFO     | openai._base_client | _sleep_for_retry:1071 | Retrying request to /chat/completions in 0.952460 seconds
2026-01-10 19:26:07 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-4901e232-01b3-44b0-a667-c4a716a65c73', 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: The Count of Monte Cristo\nCharacter: Noirtier\nBackstory: Villefort’s drift toward the royalists disappointed him; father and son argued politics at every family gathering.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'stream': False}}
2026-01-10 19:26:07 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 19:26:07 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 19:26:07 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 19:26:07 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 19:26:07 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 19:26:07 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 19:26:08 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 13:56:08 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbcad0e5ca9c0e2-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 19:26:08 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 19:26:08 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 19:26:08 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 19:26:08 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 19:26:08 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 19:26:08 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 13:56:08 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbcad0e5ca9c0e2-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 19:26:08 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 19:26:08 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 19:26:08 | DEBUG    | openai._base_client | request:1046 | Re-raising status error
2026-01-10 19:26:08 | ERROR    | __main__ | main:231 | Error processing row 4: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1768089600000'}, 'provider_name': None}}, 'user_id': 'user_2yUa3CBBFo0pOw0zUiHXS4PLFt0'}
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/main.py", line 212, in main
    final_state = run_pipeline_for_row(row_data)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/main.py", line 142, in run_pipeline_for_row
    final_state = app.invoke(initial_state)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py", line 3068, in invoke
    for chunk in self.stream(
                 ^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py", line 2643, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py", line 167, in tick
    run_with_retry(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py", line 656, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py", line 400, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/extraction_agent/main.py", line 60, in extract
    response = llm.invoke(prompt)
               ^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 402, in invoke
    self.generate_prompt(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 931, in generate
    self._generate_with_cache(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py", line 1386, in _generate
    raise e
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py", line 1381, in _generate
    raw_response = self.client.with_raw_response.create(**payload)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1768089600000'}, 'provider_name': None}}, 'user_id': 'user_2yUa3CBBFo0pOw0zUiHXS4PLFt0'}
During task with name 'extract' and id '5869412a-ad2f-4d07-d522-d51f7d557e4b'
2026-01-10 19:26:08 | INFO     | __main__ | run_pipeline_for_row:120 | Processing: Noirtier from The Count of Monte Cristo
2026-01-10 19:26:08 | INFO     | __main__ | run_pipeline_for_row:139 | Running pipeline
2026-01-10 19:26:08 | INFO     | extraction_agent.main | extract:42 | Starting extraction agent
2026-01-10 19:26:08 | INFO     | extraction_agent.main | extract:43 | Book: The Count of Monte Cristo, Character: Noirtier
2026-01-10 19:26:08 | DEBUG    | extraction_agent.main | extract:56 | Extraction prompt: You are an extractor agent.
Generate 5 queries to retrieve evidence about the character.

Book: The Count of Monte Cristo
Character: Noirtier
Backstory: His parents were targeted in a reprisal for supporting the Revolution; his mother was killed, deepening his distrust of authority.

Return only a JSON list of strings, each string being a query.
Example: ["query 1", "query 2", "query 3"]

2026-01-10 19:26:08 | INFO     | extraction_agent.main | extract:59 | Generating queries via LLM
2026-01-10 19:26:08 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-814f38aa-89d2-417f-9873-23676a38d42b', 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: The Count of Monte Cristo\nCharacter: Noirtier\nBackstory: His parents were targeted in a reprisal for supporting the Revolution; his mother was killed, deepening his distrust of authority.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'stream': False}}
2026-01-10 19:26:08 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 19:26:08 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 19:26:08 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 19:26:08 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 19:26:08 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 19:26:08 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 19:26:08 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 13:56:08 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbcad116b19c0e2-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 19:26:08 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 19:26:08 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 19:26:08 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 19:26:08 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 19:26:08 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 19:26:08 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 13:56:08 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbcad116b19c0e2-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 19:26:08 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 19:26:08 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 19:26:08 | DEBUG    | openai._base_client | _should_retry:774 | Retrying due to status code 429
2026-01-10 19:26:08 | DEBUG    | openai._base_client | _sleep_for_retry:1068 | 2 retries left
2026-01-10 19:26:08 | INFO     | openai._base_client | _sleep_for_retry:1071 | Retrying request to /chat/completions in 0.392192 seconds
2026-01-10 19:26:09 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-814f38aa-89d2-417f-9873-23676a38d42b', 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: The Count of Monte Cristo\nCharacter: Noirtier\nBackstory: His parents were targeted in a reprisal for supporting the Revolution; his mother was killed, deepening his distrust of authority.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'stream': False}}
2026-01-10 19:26:09 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 19:26:09 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 19:26:09 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 19:26:09 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 19:26:09 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 19:26:09 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 19:26:09 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 13:56:09 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbcad15cbe3c0e2-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 19:26:09 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 19:26:09 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 19:26:09 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 19:26:09 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 19:26:09 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 19:26:09 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 13:56:09 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbcad15cbe3c0e2-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 19:26:09 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 19:26:09 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 19:26:09 | DEBUG    | openai._base_client | _should_retry:774 | Retrying due to status code 429
2026-01-10 19:26:09 | DEBUG    | openai._base_client | _sleep_for_retry:1066 | 1 retry left
2026-01-10 19:26:09 | INFO     | openai._base_client | _sleep_for_retry:1071 | Retrying request to /chat/completions in 0.828154 seconds
2026-01-10 19:26:10 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-814f38aa-89d2-417f-9873-23676a38d42b', 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: The Count of Monte Cristo\nCharacter: Noirtier\nBackstory: His parents were targeted in a reprisal for supporting the Revolution; his mother was killed, deepening his distrust of authority.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'stream': False}}
2026-01-10 19:26:10 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 19:26:10 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 19:26:10 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 19:26:10 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 19:26:10 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 19:26:10 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 19:26:10 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 13:56:10 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbcad1d8cc6c0e2-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 19:26:10 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 19:26:10 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 19:26:10 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 19:26:10 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 19:26:10 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 19:26:10 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 13:56:10 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbcad1d8cc6c0e2-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 19:26:10 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 19:26:10 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 19:26:10 | DEBUG    | openai._base_client | request:1046 | Re-raising status error
2026-01-10 19:26:10 | ERROR    | __main__ | main:231 | Error processing row 5: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1768089600000'}, 'provider_name': None}}, 'user_id': 'user_2yUa3CBBFo0pOw0zUiHXS4PLFt0'}
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/main.py", line 212, in main
    final_state = run_pipeline_for_row(row_data)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/main.py", line 142, in run_pipeline_for_row
    final_state = app.invoke(initial_state)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py", line 3068, in invoke
    for chunk in self.stream(
                 ^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py", line 2643, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py", line 167, in tick
    run_with_retry(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py", line 656, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py", line 400, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/extraction_agent/main.py", line 60, in extract
    response = llm.invoke(prompt)
               ^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 402, in invoke
    self.generate_prompt(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 931, in generate
    self._generate_with_cache(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py", line 1386, in _generate
    raise e
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py", line 1381, in _generate
    raw_response = self.client.with_raw_response.create(**payload)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1768089600000'}, 'provider_name': None}}, 'user_id': 'user_2yUa3CBBFo0pOw0zUiHXS4PLFt0'}
During task with name 'extract' and id 'bdfd8d53-b099-499d-3277-5fee64dcb8de'
2026-01-10 19:26:10 | WARNING  | __main__ | main:258 | No results to save.
