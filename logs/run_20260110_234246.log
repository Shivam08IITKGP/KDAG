2026-01-10 23:42:46 | INFO     | root | setup_logging:74 | Logging initialized. Log file: logs/run_20260110_234246.log
2026-01-10 23:42:46 | INFO     | __main__ | main:168 | Starting Evidence-Grounded Backstory Consistency System
2026-01-10 23:42:55 | INFO     | __main__ | main:180 | Loaded 5 rows from train.csv
2026-01-10 23:42:55 | INFO     | __main__ | main:181 | Processing 5 backstories
2026-01-10 23:42:55 | INFO     | __main__ | main:191 | Initialized output.csv
2026-01-10 23:42:55 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:42:55 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x126f53920>
2026-01-10 23:42:55 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'GET']>
2026-01-10 23:42:55 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:42:55 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'GET']>
2026-01-10 23:42:55 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:42:55 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'GET']>
2026-01-10 23:42:56 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-encoding', b'zstd'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-type', b'application/json'), (b'date', b'Sat, 10 Jan 2026 18:12:55 GMT')])
2026-01-10 23:42:56 | INFO     | httpx | _send_single_request:1025 | HTTP Request: GET http://localhost:6333 "HTTP/1.1 200 OK"
2026-01-10 23:42:56 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'GET']>
2026-01-10 23:42:56 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:42:56 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:42:56 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:42:56 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:42:56 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:42:56 | INFO     | sentence_transformers.SentenceTransformer | __init__:219 | Use pytorch device_name: mps
2026-01-10 23:42:56 | INFO     | sentence_transformers.SentenceTransformer | __init__:227 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2026-01-10 23:42:56 | DEBUG    | urllib3.connectionpool | _new_conn:1049 | Starting new HTTPS connection (1): huggingface.co:443
2026-01-10 23:42:56 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-10 23:42:56 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-10 23:42:56 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-10 23:42:56 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-10 23:42:57 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-10 23:42:57 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-10 23:42:57 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-10 23:42:57 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-10 23:42:57 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-10 23:42:57 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-10 23:42:57 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-10 23:42:57 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-10 23:42:58 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-10 23:42:58 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-10 23:42:58 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-10 23:42:58 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-10 23:42:59 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-10 23:42:59 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-10 23:42:59 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-10 23:42:59 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-10 23:42:59 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-10 23:43:00 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6908
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12711b080>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'GET']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'GET']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'GET']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-type', b'application/json'), (b'content-encoding', b'zstd'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'date', b'Sat, 10 Jan 2026 18:13:05 GMT')])
2026-01-10 23:43:06 | INFO     | httpx | _send_single_request:1025 | HTTP Request: GET http://localhost:6333 "HTTP/1.1 200 OK"
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'GET']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x126f53320>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'GET']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'GET']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'GET']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-encoding', b'zstd'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-type', b'application/json'), (b'date', b'Sat, 10 Jan 2026 18:13:05 GMT')])
2026-01-10 23:43:06 | INFO     | httpx | _send_single_request:1025 | HTTP Request: GET http://localhost:6333/collections "HTTP/1.1 200 OK"
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'GET']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x126d6f950>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'DELETE']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'DELETE']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'DELETE']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-encoding', b'zstd'), (b'content-type', b'application/json'), (b'date', b'Sat, 10 Jan 2026 18:13:05 GMT')])
2026-01-10 23:43:06 | INFO     | httpx | _send_single_request:1025 | HTTP Request: DELETE http://localhost:6333/collections/in%20search%20of%20the%20castaways_collection "HTTP/1.1 200 OK"
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'DELETE']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12711abd0>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'PUT']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-type', b'application/json'), (b'content-encoding', b'zstd'), (b'date', b'Sat, 10 Jan 2026 18:13:05 GMT')])
2026-01-10 23:43:06 | INFO     | httpx | _send_single_request:1025 | HTTP Request: PUT http://localhost:6333/collections/in%20search%20of%20the%20castaways_collection "HTTP/1.1 200 OK"
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'PUT']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x126f53e00>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'PUT']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-type', b'application/json'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-encoding', b'zstd'), (b'date', b'Sat, 10 Jan 2026 18:13:06 GMT')])
2026-01-10 23:43:06 | INFO     | httpx | _send_single_request:1025 | HTTP Request: PUT http://localhost:6333/collections/in%20search%20of%20the%20castaways_collection/points?wait=true "HTTP/1.1 200 OK"
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'PUT']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x126f532c0>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'PUT']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-encoding', b'zstd'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-type', b'application/json'), (b'date', b'Sat, 10 Jan 2026 18:13:06 GMT')])
2026-01-10 23:43:06 | INFO     | httpx | _send_single_request:1025 | HTTP Request: PUT http://localhost:6333/collections/in%20search%20of%20the%20castaways_collection/points?wait=true "HTTP/1.1 200 OK"
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'PUT']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12711ac00>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'PUT']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-type', b'application/json'), (b'content-encoding', b'zstd'), (b'date', b'Sat, 10 Jan 2026 18:13:06 GMT')])
2026-01-10 23:43:06 | INFO     | httpx | _send_single_request:1025 | HTTP Request: PUT http://localhost:6333/collections/in%20search%20of%20the%20castaways_collection/points?wait=true "HTTP/1.1 200 OK"
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'PUT']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x127118200>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'GET']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'GET']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'GET']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-type', b'application/json'), (b'content-encoding', b'zstd'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'date', b'Sat, 10 Jan 2026 18:13:06 GMT')])
2026-01-10 23:43:06 | INFO     | httpx | _send_single_request:1025 | HTTP Request: GET http://localhost:6333 "HTTP/1.1 200 OK"
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'GET']>
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:06 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:06 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:06 | INFO     | sentence_transformers.SentenceTransformer | __init__:219 | Use pytorch device_name: mps
2026-01-10 23:43:06 | INFO     | sentence_transformers.SentenceTransformer | __init__:227 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2026-01-10 23:43:06 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-10 23:43:06 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-10 23:43:06 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-10 23:43:07 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-10 23:43:07 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-10 23:43:07 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-10 23:43:07 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-10 23:43:07 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-10 23:43:07 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-10 23:43:07 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-10 23:43:08 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-10 23:43:08 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-10 23:43:08 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-10 23:43:08 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-10 23:43:08 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-10 23:43:09 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-10 23:43:09 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-10 23:43:09 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-10 23:43:09 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-10 23:43:09 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-10 23:43:10 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-10 23:43:10 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6908
2026-01-10 23:43:10 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:43:10 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1274513a0>
2026-01-10 23:43:10 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 23:43:10 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:10 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 23:43:10 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:10 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 23:43:10 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-encoding', b'zstd'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-type', b'application/json'), (b'date', b'Sat, 10 Jan 2026 18:13:10 GMT')])
2026-01-10 23:43:10 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST http://localhost:6333/collections/in%20search%20of%20the%20castaways_collection/points/query "HTTP/1.1 200 OK"
2026-01-10 23:43:10 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 23:43:10 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:10 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:10 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:10 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:10 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:10 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-10 23:43:11 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-10 23:43:11 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/config.json HTTP/1.1" 200 0
2026-01-10 23:43:11 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-10 23:43:11 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-10 23:43:12 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/tokenizer_config.json HTTP/1.1" 200 0
2026-01-10 23:43:12 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "GET /api/models/cross-encoder/ms-marco-MiniLM-L-6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 307 147
2026-01-10 23:43:12 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "GET /api/models/cross-encoder/ms-marco-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-10 23:43:12 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "GET /api/models/cross-encoder/ms-marco-MiniLM-L-6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 307 120
2026-01-10 23:43:13 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "GET /api/models/cross-encoder/ms-marco-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 5436
2026-01-10 23:43:13 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-10 23:43:13 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-10 23:43:13 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/README.md HTTP/1.1" 200 0
2026-01-10 23:43:13 | INFO     | sentence_transformers.cross_encoder.CrossEncoder | __init__:228 | Use pytorch device: mps
2026-01-10 23:43:14 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "GET /api/models/cross-encoder/ms-marco-MiniLM-L-6-v2 HTTP/1.1" 307 82
2026-01-10 23:43:14 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "GET /api/models/cross-encoder/ms-marco-MiniLM-L6-v2 HTTP/1.1" 200 5370
2026-01-10 23:43:14 | INFO     | __main__ | run_pipeline_for_row:135 | Processing: Thalcave from In Search of the Castaways
2026-01-10 23:43:14 | INFO     | __main__ | run_pipeline_for_row:154 | Running pipeline
2026-01-10 23:43:14 | INFO     | extraction_agent.main | extract:47 | Starting extraction agent
2026-01-10 23:43:14 | INFO     | extraction_agent.main | extract:48 | Book: In Search of the Castaways, Character: Thalcave
2026-01-10 23:43:14 | DEBUG    | extraction_agent.main | extract:61 | Extraction prompt: You are an extractor agent.
Generate 5 queries to retrieve evidence about the character.

Book: In Search of the Castaways
Character: Thalcave
Backstory: Thalcave’s people faded as colonists advanced; his father, last of the tribal guides, knew the pampas geography and animal ways, while his mother died giving birth. Boyhood was spent roaming the plains with his father, learning to track, tame horses and steer by the stars.

Return only a JSON list of strings, each string being a query.
Example: ["query 1", "query 2", "query 3"]

2026-01-10 23:43:14 | INFO     | extraction_agent.main | extract:64 | Generating queries via LLM
2026-01-10 23:43:14 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Helper-Method': 'chat.completions.parse', 'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-c91f1680-ffb7-46dc-8db2-bf4675f2d3bf', 'post_parser': <function Completions.parse.<locals>.parser at 0x12736e700>, 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: In Search of the Castaways\nCharacter: Thalcave\nBackstory: Thalcave’s people faded as colonists advanced; his father, last of the tribal guides, knew the pampas geography and animal ways, while his mother died giving birth. Boyhood was spent roaming the plains with his father, learning to track, tame horses and steer by the stars.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'QueryListOutput', 'description': 'Structured output format for query extraction.', 'strict': False, 'schema': {'type': 'object', 'properties': {'queries': {'type': 'array', 'items': {'type': 'string'}}}, 'required': ['queries']}}}, 'stream': False}}
2026-01-10 23:43:14 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 23:43:14 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=None socket_options=None
2026-01-10 23:43:15 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x127363320>
2026-01-10 23:43:15 | DEBUG    | httpcore.connection | trace:47 | start_tls.started ssl_context=<ssl.SSLContext object at 0x137b3efd0> server_hostname='openrouter.ai' timeout=None
2026-01-10 23:43:15 | DEBUG    | httpcore.connection | trace:47 | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12751bbf0>
2026-01-10 23:43:15 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 23:43:15 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:15 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 23:43:15 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:15 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 23:43:15 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 18:13:15 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbe25b26b45bf12-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 23:43:15 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 23:43:15 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 23:43:15 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:15 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:15 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:15 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 18:13:15 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbe25b26b45bf12-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 23:43:15 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 23:43:15 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 23:43:15 | DEBUG    | openai._base_client | _should_retry:774 | Retrying due to status code 429
2026-01-10 23:43:15 | DEBUG    | openai._base_client | _sleep_for_retry:1068 | 2 retries left
2026-01-10 23:43:15 | INFO     | openai._base_client | _sleep_for_retry:1071 | Retrying request to /chat/completions in 0.433505 seconds
2026-01-10 23:43:16 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Helper-Method': 'chat.completions.parse', 'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-c91f1680-ffb7-46dc-8db2-bf4675f2d3bf', 'post_parser': <function Completions.parse.<locals>.parser at 0x12736e700>, 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: In Search of the Castaways\nCharacter: Thalcave\nBackstory: Thalcave’s people faded as colonists advanced; his father, last of the tribal guides, knew the pampas geography and animal ways, while his mother died giving birth. Boyhood was spent roaming the plains with his father, learning to track, tame horses and steer by the stars.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'QueryListOutput', 'description': 'Structured output format for query extraction.', 'strict': False, 'schema': {'type': 'object', 'properties': {'queries': {'type': 'array', 'items': {'type': 'string'}}}, 'required': ['queries']}}}, 'stream': False}}
2026-01-10 23:43:16 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 23:43:16 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 23:43:16 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:16 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 23:43:16 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:16 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 23:43:16 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 18:13:16 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbe25b92f21bf12-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 23:43:16 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 23:43:16 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 23:43:16 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:16 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:16 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:16 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 18:13:16 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbe25b92f21bf12-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 23:43:16 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 23:43:16 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 23:43:16 | DEBUG    | openai._base_client | _should_retry:774 | Retrying due to status code 429
2026-01-10 23:43:16 | DEBUG    | openai._base_client | _sleep_for_retry:1066 | 1 retry left
2026-01-10 23:43:16 | INFO     | openai._base_client | _sleep_for_retry:1071 | Retrying request to /chat/completions in 0.985365 seconds
2026-01-10 23:43:17 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Helper-Method': 'chat.completions.parse', 'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-c91f1680-ffb7-46dc-8db2-bf4675f2d3bf', 'post_parser': <function Completions.parse.<locals>.parser at 0x12736e700>, 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: In Search of the Castaways\nCharacter: Thalcave\nBackstory: Thalcave’s people faded as colonists advanced; his father, last of the tribal guides, knew the pampas geography and animal ways, while his mother died giving birth. Boyhood was spent roaming the plains with his father, learning to track, tame horses and steer by the stars.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'QueryListOutput', 'description': 'Structured output format for query extraction.', 'strict': False, 'schema': {'type': 'object', 'properties': {'queries': {'type': 'array', 'items': {'type': 'string'}}}, 'required': ['queries']}}}, 'stream': False}}
2026-01-10 23:43:17 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 23:43:17 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 23:43:17 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:17 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 23:43:17 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:17 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 23:43:17 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 18:13:17 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbe25c14c5cbf12-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 23:43:17 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 23:43:17 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 23:43:17 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:17 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:17 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:17 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 18:13:17 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbe25c14c5cbf12-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 23:43:17 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 23:43:17 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 23:43:17 | DEBUG    | openai._base_client | request:1046 | Re-raising status error
2026-01-10 23:43:17 | WARNING  | extraction_agent.main | extract:79 | Structured output failed: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1768089600000'}, 'provider_name': None}}, 'user_id': 'user_2yUa3CBBFo0pOw0zUiHXS4PLFt0'}. Falling back to JSON parsing
2026-01-10 23:43:17 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-d4e1190b-3003-4f35-8dd4-0919bdc5eb15', 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: In Search of the Castaways\nCharacter: Thalcave\nBackstory: Thalcave’s people faded as colonists advanced; his father, last of the tribal guides, knew the pampas geography and animal ways, while his mother died giving birth. Boyhood was spent roaming the plains with his father, learning to track, tame horses and steer by the stars.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'stream': False}}
2026-01-10 23:43:17 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 23:43:17 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 23:43:17 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:17 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 23:43:17 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:17 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 23:43:18 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 18:13:18 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbe25c50b39bf12-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 23:43:18 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 23:43:18 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 23:43:18 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:18 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:18 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:18 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 18:13:18 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbe25c50b39bf12-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 23:43:18 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 23:43:18 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/extraction_agent/main.py", line 70, in extract
    response_data: QueryListOutput = structured_llm.invoke(prompt)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py", line 3149, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py", line 5557, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 402, in invoke
    self.generate_prompt(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 931, in generate
    self._generate_with_cache(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py", line 1386, in _generate
    raise e
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py", line 1354, in _generate
    self.root_client.chat.completions.with_raw_response.parse(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 184, in parse
    return self._post(
           ^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1768089600000'}, 'provider_name': None}}, 'user_id': 'user_2yUa3CBBFo0pOw0zUiHXS4PLFt0'}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 23:43:18 | DEBUG    | openai._base_client | _should_retry:774 | Retrying due to status code 429
2026-01-10 23:43:18 | DEBUG    | openai._base_client | _sleep_for_retry:1068 | 2 retries left
2026-01-10 23:43:18 | INFO     | openai._base_client | _sleep_for_retry:1071 | Retrying request to /chat/completions in 0.458445 seconds
2026-01-10 23:43:18 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-d4e1190b-3003-4f35-8dd4-0919bdc5eb15', 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: In Search of the Castaways\nCharacter: Thalcave\nBackstory: Thalcave’s people faded as colonists advanced; his father, last of the tribal guides, knew the pampas geography and animal ways, while his mother died giving birth. Boyhood was spent roaming the plains with his father, learning to track, tame horses and steer by the stars.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'stream': False}}
2026-01-10 23:43:18 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 23:43:18 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 23:43:18 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:18 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 23:43:18 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:18 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 23:43:19 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 18:13:19 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbe25ca1caebf12-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 23:43:19 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 23:43:19 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 23:43:19 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:19 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:19 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:19 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 18:13:19 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbe25ca1caebf12-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 23:43:19 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 23:43:19 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/extraction_agent/main.py", line 70, in extract
    response_data: QueryListOutput = structured_llm.invoke(prompt)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py", line 3149, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py", line 5557, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 402, in invoke
    self.generate_prompt(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 931, in generate
    self._generate_with_cache(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py", line 1386, in _generate
    raise e
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py", line 1354, in _generate
    self.root_client.chat.completions.with_raw_response.parse(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 184, in parse
    return self._post(
           ^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1768089600000'}, 'provider_name': None}}, 'user_id': 'user_2yUa3CBBFo0pOw0zUiHXS4PLFt0'}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 23:43:19 | DEBUG    | openai._base_client | _should_retry:774 | Retrying due to status code 429
2026-01-10 23:43:19 | DEBUG    | openai._base_client | _sleep_for_retry:1066 | 1 retry left
2026-01-10 23:43:19 | INFO     | openai._base_client | _sleep_for_retry:1071 | Retrying request to /chat/completions in 0.763671 seconds
2026-01-10 23:43:19 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-d4e1190b-3003-4f35-8dd4-0919bdc5eb15', 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: In Search of the Castaways\nCharacter: Thalcave\nBackstory: Thalcave’s people faded as colonists advanced; his father, last of the tribal guides, knew the pampas geography and animal ways, while his mother died giving birth. Boyhood was spent roaming the plains with his father, learning to track, tame horses and steer by the stars.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'stream': False}}
2026-01-10 23:43:19 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 23:43:19 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 23:43:19 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:19 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 23:43:19 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:19 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 23:43:20 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 18:13:20 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbe25d0fc12bf12-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 23:43:20 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 23:43:20 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 23:43:20 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:20 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:20 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:20 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 18:13:20 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbe25d0fc12bf12-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 23:43:20 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 23:43:20 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/extraction_agent/main.py", line 70, in extract
    response_data: QueryListOutput = structured_llm.invoke(prompt)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py", line 3149, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py", line 5557, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 402, in invoke
    self.generate_prompt(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 931, in generate
    self._generate_with_cache(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py", line 1386, in _generate
    raise e
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py", line 1354, in _generate
    self.root_client.chat.completions.with_raw_response.parse(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 184, in parse
    return self._post(
           ^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1768089600000'}, 'provider_name': None}}, 'user_id': 'user_2yUa3CBBFo0pOw0zUiHXS4PLFt0'}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 23:43:20 | DEBUG    | openai._base_client | request:1046 | Re-raising status error
2026-01-10 23:43:20 | ERROR    | extraction_agent.main | extract:118 | Fallback parsing also failed: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1768089600000'}, 'provider_name': None}}, 'user_id': 'user_2yUa3CBBFo0pOw0zUiHXS4PLFt0'}
2026-01-10 23:43:20 | ERROR    | extraction_agent.main | extract:119 | Response text: N/A
2026-01-10 23:43:20 | INFO     | extraction_agent.main | extract:137 | Retrieved 0 evidence items
2026-01-10 23:43:20 | INFO     | extraction_agent.main | extract:150 | Extraction agent completed
2026-01-10 23:43:20 | INFO     | graph_creator_agent.main | create_graph:26 | Starting graph creator agent
2026-01-10 23:43:20 | INFO     | graph_creator_agent.main | create_graph:27 | Book: In Search of the Castaways, Character: Thalcave
2026-01-10 23:43:20 | INFO     | graph_creator_agent.graph_store | load_graph:79 | Loading existing graph from graph_creator_agent/graph/In Search of the Castaways_Thalcave.graphml
2026-01-10 23:43:20 | INFO     | graph_creator_agent.graph_store | load_graph:84 | Loaded graph with 169 nodes and 156 edges
2026-01-10 23:43:20 | INFO     | graph_creator_agent.utils | filter_new_evidence:22 | Filtered evidence: 0 new out of 0 total
2026-01-10 23:43:20 | INFO     | graph_creator_agent.main | create_graph:43 | No new evidence to process
2026-01-10 23:43:20 | INFO     | answering_agent.main | answer:21 | Starting answering agent
2026-01-10 23:43:20 | INFO     | answering_agent.main | answer:22 | Book: In Search of the Castaways, Character: Thalcave
2026-01-10 23:43:20 | INFO     | answering_agent.main | answer:33 | Running classifier
2026-01-10 23:43:20 | INFO     | answering_agent.classifier | classify:69 | Starting classification
2026-01-10 23:43:20 | INFO     | answering_agent.classifier | classify:79 | Loaded graph from graph_creator_agent/graph/In Search of the Castaways_Thalcave.graphml
2026-01-10 23:43:20 | DEBUG    | answering_agent.classifier | classify:93 | Classification prompt: 
You are a verification agent that checks whether a character backstory is CONSISTENT with a given knowledge graph extracted from a novel.

IMPORTANT RULES:
- Use ONLY the provided knowledge graph sum...
2026-01-10 23:43:20 | INFO     | answering_agent.classifier | classify:96 | Calling LLM for classification
2026-01-10 23:43:20 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-75af30c0-91e7-4783-affe-744b959bf7aa', 'json_data': {'messages': [{'content': '\nYou are a verification agent that checks whether a character backstory is CONSISTENT with a given knowledge graph extracted from a novel.\n\nIMPORTANT RULES:\n- Use ONLY the provided knowledge graph summary.\n- Do NOT use outside knowledge or assumptions.\n- The knowledge graph represents ground-truth facts from the book.\n- If the backstory directly contradicts any graph fact, label = 0.\n- If the backstory is fully supported or not contradicted, label = 1.\n- Missing information is NOT a contradiction.\n\nINPUTS:\nBook: In Search of the Castaways\nCharacter: Thalcave\n\nBackstory to Verify:\nThalcave’s people faded as colonists advanced; his father, last of the tribal guides, knew the pampas geography and animal ways, while his mother died giving birth. Boyhood was spent roaming the plains with his father, learning to track, tame horses and steer by the stars.\n\nKnowledge Graph Summary (facts and relations):\nGraph has 169 nodes and 156 edges.\n\nKey relationships:\nJacques Paganel --[IS_GEOGRAPHER]--> French\nJacques Paganel --[HAS_TRAIT]--> absent-minded\nJacques Paganel --[KNOWN_FOR]--> absent-mindedness\nJacques Paganel --[EXPERT_IN]--> geography\nJacques Paganel --[TRAVELED_TO]--> world\nJacques Paganel --[HAS_TRAIT]--> highly knowledgeable\nJacques Paganel --[KNOWN_FOR]--> frequent forgetfulness\nJacques Paganel --[PERSONALITY_IS]--> forgetful\nJacques Paganel --[MEMBER_OF]--> Geographical Society\nJacques Paganel --[WRITES]--> scholarly papers\n\nDECISION CRITERIA:\n1. Factual consistency: Are all claims compatible with graph facts?\n2. Temporal/causal consistency: Do events align logically with known causes and outcomes?\n3. Narrative consistency: Does the backstory violate established story constraints?\n\nOUTPUT FORMAT (strict JSON):\n{\n  "label": 1 or 0,\n  "reasoning": "Step-by-step justification explicitly referencing graph facts or stating absence of contradiction.",\n  "evidence_queries": [\n    "Query 1 to find exact verbatim evidence from novel",\n    "Query 2 to find supporting passages",\n    "Query 3 to verify specific claims"\n  ]\n}\n\nIMPORTANT: Generate 3-5 specific queries that would retrieve exact text passages from the novel \nthat justify your reasoning. These should be precise search queries targeting specific facts, \nevents, or character descriptions mentioned in your reasoning.\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'stream': False}}
2026-01-10 23:43:20 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 23:43:20 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 23:43:20 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:20 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 23:43:20 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:20 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 23:43:20 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 18:13:20 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbe25d35812bf12-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 23:43:20 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 23:43:20 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 23:43:20 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:20 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:20 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:20 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 18:13:20 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbe25d35812bf12-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 23:43:20 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 23:43:20 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 23:43:20 | DEBUG    | openai._base_client | _should_retry:774 | Retrying due to status code 429
2026-01-10 23:43:20 | DEBUG    | openai._base_client | _sleep_for_retry:1068 | 2 retries left
2026-01-10 23:43:20 | INFO     | openai._base_client | _sleep_for_retry:1071 | Retrying request to /chat/completions in 0.485578 seconds
2026-01-10 23:43:21 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-75af30c0-91e7-4783-affe-744b959bf7aa', 'json_data': {'messages': [{'content': '\nYou are a verification agent that checks whether a character backstory is CONSISTENT with a given knowledge graph extracted from a novel.\n\nIMPORTANT RULES:\n- Use ONLY the provided knowledge graph summary.\n- Do NOT use outside knowledge or assumptions.\n- The knowledge graph represents ground-truth facts from the book.\n- If the backstory directly contradicts any graph fact, label = 0.\n- If the backstory is fully supported or not contradicted, label = 1.\n- Missing information is NOT a contradiction.\n\nINPUTS:\nBook: In Search of the Castaways\nCharacter: Thalcave\n\nBackstory to Verify:\nThalcave’s people faded as colonists advanced; his father, last of the tribal guides, knew the pampas geography and animal ways, while his mother died giving birth. Boyhood was spent roaming the plains with his father, learning to track, tame horses and steer by the stars.\n\nKnowledge Graph Summary (facts and relations):\nGraph has 169 nodes and 156 edges.\n\nKey relationships:\nJacques Paganel --[IS_GEOGRAPHER]--> French\nJacques Paganel --[HAS_TRAIT]--> absent-minded\nJacques Paganel --[KNOWN_FOR]--> absent-mindedness\nJacques Paganel --[EXPERT_IN]--> geography\nJacques Paganel --[TRAVELED_TO]--> world\nJacques Paganel --[HAS_TRAIT]--> highly knowledgeable\nJacques Paganel --[KNOWN_FOR]--> frequent forgetfulness\nJacques Paganel --[PERSONALITY_IS]--> forgetful\nJacques Paganel --[MEMBER_OF]--> Geographical Society\nJacques Paganel --[WRITES]--> scholarly papers\n\nDECISION CRITERIA:\n1. Factual consistency: Are all claims compatible with graph facts?\n2. Temporal/causal consistency: Do events align logically with known causes and outcomes?\n3. Narrative consistency: Does the backstory violate established story constraints?\n\nOUTPUT FORMAT (strict JSON):\n{\n  "label": 1 or 0,\n  "reasoning": "Step-by-step justification explicitly referencing graph facts or stating absence of contradiction.",\n  "evidence_queries": [\n    "Query 1 to find exact verbatim evidence from novel",\n    "Query 2 to find supporting passages",\n    "Query 3 to verify specific claims"\n  ]\n}\n\nIMPORTANT: Generate 3-5 specific queries that would retrieve exact text passages from the novel \nthat justify your reasoning. These should be precise search queries targeting specific facts, \nevents, or character descriptions mentioned in your reasoning.\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'stream': False}}
2026-01-10 23:43:21 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 23:43:21 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 23:43:21 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:21 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 23:43:21 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:21 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 23:43:21 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 18:13:21 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbe25d8794cbf12-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 23:43:21 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 23:43:21 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 23:43:21 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:21 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:21 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:21 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 18:13:21 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbe25d8794cbf12-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 23:43:21 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 23:43:21 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 23:43:21 | DEBUG    | openai._base_client | _should_retry:774 | Retrying due to status code 429
2026-01-10 23:43:21 | DEBUG    | openai._base_client | _sleep_for_retry:1066 | 1 retry left
2026-01-10 23:43:21 | INFO     | openai._base_client | _sleep_for_retry:1071 | Retrying request to /chat/completions in 0.881525 seconds
2026-01-10 23:43:22 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-75af30c0-91e7-4783-affe-744b959bf7aa', 'json_data': {'messages': [{'content': '\nYou are a verification agent that checks whether a character backstory is CONSISTENT with a given knowledge graph extracted from a novel.\n\nIMPORTANT RULES:\n- Use ONLY the provided knowledge graph summary.\n- Do NOT use outside knowledge or assumptions.\n- The knowledge graph represents ground-truth facts from the book.\n- If the backstory directly contradicts any graph fact, label = 0.\n- If the backstory is fully supported or not contradicted, label = 1.\n- Missing information is NOT a contradiction.\n\nINPUTS:\nBook: In Search of the Castaways\nCharacter: Thalcave\n\nBackstory to Verify:\nThalcave’s people faded as colonists advanced; his father, last of the tribal guides, knew the pampas geography and animal ways, while his mother died giving birth. Boyhood was spent roaming the plains with his father, learning to track, tame horses and steer by the stars.\n\nKnowledge Graph Summary (facts and relations):\nGraph has 169 nodes and 156 edges.\n\nKey relationships:\nJacques Paganel --[IS_GEOGRAPHER]--> French\nJacques Paganel --[HAS_TRAIT]--> absent-minded\nJacques Paganel --[KNOWN_FOR]--> absent-mindedness\nJacques Paganel --[EXPERT_IN]--> geography\nJacques Paganel --[TRAVELED_TO]--> world\nJacques Paganel --[HAS_TRAIT]--> highly knowledgeable\nJacques Paganel --[KNOWN_FOR]--> frequent forgetfulness\nJacques Paganel --[PERSONALITY_IS]--> forgetful\nJacques Paganel --[MEMBER_OF]--> Geographical Society\nJacques Paganel --[WRITES]--> scholarly papers\n\nDECISION CRITERIA:\n1. Factual consistency: Are all claims compatible with graph facts?\n2. Temporal/causal consistency: Do events align logically with known causes and outcomes?\n3. Narrative consistency: Does the backstory violate established story constraints?\n\nOUTPUT FORMAT (strict JSON):\n{\n  "label": 1 or 0,\n  "reasoning": "Step-by-step justification explicitly referencing graph facts or stating absence of contradiction.",\n  "evidence_queries": [\n    "Query 1 to find exact verbatim evidence from novel",\n    "Query 2 to find supporting passages",\n    "Query 3 to verify specific claims"\n  ]\n}\n\nIMPORTANT: Generate 3-5 specific queries that would retrieve exact text passages from the novel \nthat justify your reasoning. These should be precise search queries targeting specific facts, \nevents, or character descriptions mentioned in your reasoning.\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'stream': False}}
2026-01-10 23:43:22 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 23:43:22 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 23:43:22 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:22 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 23:43:22 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:22 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 23:43:22 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 18:13:22 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbe25dfee25bf12-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 23:43:22 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 23:43:22 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 23:43:22 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:22 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:22 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:22 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 18:13:22 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbe25dfee25bf12-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 23:43:22 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 23:43:22 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 23:43:22 | DEBUG    | openai._base_client | request:1046 | Re-raising status error
2026-01-10 23:43:22 | ERROR    | __main__ | main:246 | Error processing row 1: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1768089600000'}, 'provider_name': None}}, 'user_id': 'user_2yUa3CBBFo0pOw0zUiHXS4PLFt0'}
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/main.py", line 227, in main
    final_state = run_pipeline_for_row(row_data)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/main.py", line 157, in run_pipeline_for_row
    final_state = app.invoke(initial_state)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py", line 3068, in invoke
    for chunk in self.stream(
                 ^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py", line 2643, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py", line 167, in tick
    run_with_retry(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py", line 656, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py", line 400, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/answering_agent/main.py", line 34, in answer
    classification: ClassificationOutput = classify(
                                           ^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/answering_agent/classifier.py", line 97, in classify
    response = llm.invoke(prompt)
               ^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 402, in invoke
    self.generate_prompt(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 931, in generate
    self._generate_with_cache(
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py", line 1386, in _generate
    raise e
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py", line 1381, in _generate
    raw_response = self.client.with_raw_response.create(**payload)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1768089600000'}, 'provider_name': None}}, 'user_id': 'user_2yUa3CBBFo0pOw0zUiHXS4PLFt0'}
During task with name 'answer' and id '76711059-5296-543e-99f9-5d5fdad14949'
2026-01-10 23:43:22 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:43:22 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1378b1dc0>
2026-01-10 23:43:22 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'GET']>
2026-01-10 23:43:22 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:22 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'GET']>
2026-01-10 23:43:22 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:22 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'GET']>
2026-01-10 23:43:22 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-type', b'application/json'), (b'content-encoding', b'zstd'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'date', b'Sat, 10 Jan 2026 18:13:22 GMT')])
2026-01-10 23:43:22 | INFO     | httpx | _send_single_request:1025 | HTTP Request: GET http://localhost:6333 "HTTP/1.1 200 OK"
2026-01-10 23:43:22 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'GET']>
2026-01-10 23:43:22 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:22 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:22 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:22 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:22 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:22 | INFO     | sentence_transformers.SentenceTransformer | __init__:219 | Use pytorch device_name: mps
2026-01-10 23:43:22 | INFO     | sentence_transformers.SentenceTransformer | __init__:227 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2026-01-10 23:43:23 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-10 23:43:23 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-10 23:43:24 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-10 23:43:24 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-10 23:43:24 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-10 23:43:24 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-10 23:43:24 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-10 23:43:24 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-10 23:43:24 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-10 23:43:24 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-10 23:43:25 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-10 23:43:25 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-10 23:43:25 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-10 23:43:25 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-10 23:43:25 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-10 23:43:26 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-10 23:43:26 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-10 23:43:26 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-10 23:43:26 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=True&expand=False HTTP/1.1" 200 6465
2026-01-10 23:43:27 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-10 23:43:27 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-10 23:43:27 | DEBUG    | urllib3.connectionpool | _make_request:544 | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6908
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1378ca780>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'GET']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'GET']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'GET']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-encoding', b'zstd'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-type', b'application/json'), (b'date', b'Sat, 10 Jan 2026 18:13:43 GMT')])
2026-01-10 23:43:43 | INFO     | httpx | _send_single_request:1025 | HTTP Request: GET http://localhost:6333 "HTTP/1.1 200 OK"
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'GET']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1378e42c0>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'GET']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'GET']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'GET']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-encoding', b'zstd'), (b'content-type', b'application/json'), (b'date', b'Sat, 10 Jan 2026 18:13:43 GMT')])
2026-01-10 23:43:43 | INFO     | httpx | _send_single_request:1025 | HTTP Request: GET http://localhost:6333/collections "HTTP/1.1 200 OK"
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'GET']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1378ca300>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'DELETE']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'DELETE']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'DELETE']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-type', b'application/json'), (b'content-encoding', b'zstd'), (b'date', b'Sat, 10 Jan 2026 18:13:43 GMT')])
2026-01-10 23:43:43 | INFO     | httpx | _send_single_request:1025 | HTTP Request: DELETE http://localhost:6333/collections/the%20count%20of%20monte%20cristo_collection "HTTP/1.1 200 OK"
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'DELETE']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1378c96d0>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-type', b'application/json'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-encoding', b'zstd'), (b'date', b'Sat, 10 Jan 2026 18:13:43 GMT')])
2026-01-10 23:43:43 | INFO     | httpx | _send_single_request:1025 | HTTP Request: PUT http://localhost:6333/collections/the%20count%20of%20monte%20cristo_collection "HTTP/1.1 200 OK"
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1378e4320>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-encoding', b'zstd'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-type', b'application/json'), (b'date', b'Sat, 10 Jan 2026 18:13:43 GMT')])
2026-01-10 23:43:43 | INFO     | httpx | _send_single_request:1025 | HTTP Request: PUT http://localhost:6333/collections/the%20count%20of%20monte%20cristo_collection/points?wait=true "HTTP/1.1 200 OK"
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1378e6a20>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-type', b'application/json'), (b'content-encoding', b'zstd'), (b'date', b'Sat, 10 Jan 2026 18:13:43 GMT')])
2026-01-10 23:43:43 | INFO     | httpx | _send_single_request:1025 | HTTP Request: PUT http://localhost:6333/collections/the%20count%20of%20monte%20cristo_collection/points?wait=true "HTTP/1.1 200 OK"
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1378e6a80>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-type', b'application/json'), (b'content-encoding', b'zstd'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'date', b'Sat, 10 Jan 2026 18:13:43 GMT')])
2026-01-10 23:43:43 | INFO     | httpx | _send_single_request:1025 | HTTP Request: PUT http://localhost:6333/collections/the%20count%20of%20monte%20cristo_collection/points?wait=true "HTTP/1.1 200 OK"
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1378e42c0>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-encoding', b'zstd'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-type', b'application/json'), (b'date', b'Sat, 10 Jan 2026 18:13:43 GMT')])
2026-01-10 23:43:43 | INFO     | httpx | _send_single_request:1025 | HTTP Request: PUT http://localhost:6333/collections/the%20count%20of%20monte%20cristo_collection/points?wait=true "HTTP/1.1 200 OK"
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1378e4860>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-type', b'application/json'), (b'content-encoding', b'zstd'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'date', b'Sat, 10 Jan 2026 18:13:43 GMT')])
2026-01-10 23:43:43 | INFO     | httpx | _send_single_request:1025 | HTTP Request: PUT http://localhost:6333/collections/the%20count%20of%20monte%20cristo_collection/points?wait=true "HTTP/1.1 200 OK"
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1378e6720>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-encoding', b'zstd'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-type', b'application/json'), (b'date', b'Sat, 10 Jan 2026 18:13:43 GMT')])
2026-01-10 23:43:43 | INFO     | httpx | _send_single_request:1025 | HTTP Request: PUT http://localhost:6333/collections/the%20count%20of%20monte%20cristo_collection/points?wait=true "HTTP/1.1 200 OK"
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1378e4860>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-encoding', b'zstd'), (b'content-type', b'application/json'), (b'date', b'Sat, 10 Jan 2026 18:13:43 GMT')])
2026-01-10 23:43:43 | INFO     | httpx | _send_single_request:1025 | HTTP Request: PUT http://localhost:6333/collections/the%20count%20of%20monte%20cristo_collection/points?wait=true "HTTP/1.1 200 OK"
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'PUT']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1378e4560>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-type', b'application/json'), (b'content-encoding', b'zstd'), (b'date', b'Sat, 10 Jan 2026 18:13:43 GMT')])
2026-01-10 23:43:43 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST http://localhost:6333/collections/the%20count%20of%20monte%20cristo_collection/points/query "HTTP/1.1 200 OK"
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:43 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:43 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:44 | INFO     | __main__ | run_pipeline_for_row:135 | Processing: Faria from The Count of Monte Cristo
2026-01-10 23:43:44 | INFO     | __main__ | run_pipeline_for_row:154 | Running pipeline
2026-01-10 23:43:44 | INFO     | extraction_agent.main | extract:47 | Starting extraction agent
2026-01-10 23:43:44 | INFO     | extraction_agent.main | extract:48 | Book: The Count of Monte Cristo, Character: Faria
2026-01-10 23:43:44 | DEBUG    | extraction_agent.main | extract:61 | Extraction prompt: You are an extractor agent.
Generate 5 queries to retrieve evidence about the character.

Book: The Count of Monte Cristo
Character: Faria
Backstory: Suspected again in 1815, he was re-arrested and shipped to the Château d’If, this time for life.

Return only a JSON list of strings, each string being a query.
Example: ["query 1", "query 2", "query 3"]

2026-01-10 23:43:44 | INFO     | extraction_agent.main | extract:64 | Generating queries via LLM
2026-01-10 23:43:44 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Helper-Method': 'chat.completions.parse', 'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-401b0c2b-45cb-4caa-92ee-880d95db1cec', 'post_parser': <function Completions.parse.<locals>.parser at 0x1377bf7e0>, 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: The Count of Monte Cristo\nCharacter: Faria\nBackstory: Suspected again in 1815, he was re-arrested and shipped to the Château d’If, this time for life.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'QueryListOutput', 'description': 'Structured output format for query extraction.', 'strict': False, 'schema': {'type': 'object', 'properties': {'queries': {'type': 'array', 'items': {'type': 'string'}}}, 'required': ['queries']}}}, 'stream': False}}
2026-01-10 23:43:44 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 23:43:44 | DEBUG    | httpcore.connection | trace:47 | close.started
2026-01-10 23:43:44 | DEBUG    | httpcore.connection | trace:47 | close.complete
2026-01-10 23:43:44 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=None socket_options=None
2026-01-10 23:43:44 | DEBUG    | httpcore.connection | trace:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1378c8aa0>
2026-01-10 23:43:44 | DEBUG    | httpcore.connection | trace:47 | start_tls.started ssl_context=<ssl.SSLContext object at 0x137b3efd0> server_hostname='openrouter.ai' timeout=None
2026-01-10 23:43:44 | DEBUG    | httpcore.connection | trace:47 | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1378c8b00>
2026-01-10 23:43:44 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 23:43:44 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:44 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 23:43:44 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:44 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 23:43:45 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 18:13:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbe266a8d9ef43e-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 23:43:45 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 23:43:45 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 23:43:45 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:45 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:45 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:45 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 18:13:45 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbe266a8d9ef43e-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 23:43:45 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 23:43:45 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 23:43:45 | DEBUG    | openai._base_client | _should_retry:774 | Retrying due to status code 429
2026-01-10 23:43:45 | DEBUG    | openai._base_client | _sleep_for_retry:1068 | 2 retries left
2026-01-10 23:43:45 | INFO     | openai._base_client | _sleep_for_retry:1071 | Retrying request to /chat/completions in 0.432819 seconds
2026-01-10 23:43:45 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Helper-Method': 'chat.completions.parse', 'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-401b0c2b-45cb-4caa-92ee-880d95db1cec', 'post_parser': <function Completions.parse.<locals>.parser at 0x1377bf7e0>, 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: The Count of Monte Cristo\nCharacter: Faria\nBackstory: Suspected again in 1815, he was re-arrested and shipped to the Château d’If, this time for life.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'QueryListOutput', 'description': 'Structured output format for query extraction.', 'strict': False, 'schema': {'type': 'object', 'properties': {'queries': {'type': 'array', 'items': {'type': 'string'}}}, 'required': ['queries']}}}, 'stream': False}}
2026-01-10 23:43:45 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 23:43:45 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 23:43:45 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:45 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 23:43:45 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:45 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 23:43:45 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 18:13:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbe2670a947f43e-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 23:43:45 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 23:43:45 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 23:43:45 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:45 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:45 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:45 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 18:13:45 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbe2670a947f43e-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 23:43:45 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 23:43:45 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 23:43:45 | DEBUG    | openai._base_client | _should_retry:774 | Retrying due to status code 429
2026-01-10 23:43:45 | DEBUG    | openai._base_client | _sleep_for_retry:1066 | 1 retry left
2026-01-10 23:43:45 | INFO     | openai._base_client | _sleep_for_retry:1071 | Retrying request to /chat/completions in 0.828615 seconds
2026-01-10 23:43:46 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Helper-Method': 'chat.completions.parse', 'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-401b0c2b-45cb-4caa-92ee-880d95db1cec', 'post_parser': <function Completions.parse.<locals>.parser at 0x1377bf7e0>, 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: The Count of Monte Cristo\nCharacter: Faria\nBackstory: Suspected again in 1815, he was re-arrested and shipped to the Château d’If, this time for life.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'QueryListOutput', 'description': 'Structured output format for query extraction.', 'strict': False, 'schema': {'type': 'object', 'properties': {'queries': {'type': 'array', 'items': {'type': 'string'}}}, 'required': ['queries']}}}, 'stream': False}}
2026-01-10 23:43:46 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 23:43:46 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 23:43:46 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:46 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 23:43:46 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:46 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 23:43:47 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 10 Jan 2026 18:13:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'9bbe26789832f43e-DEL'), (b'Access-Control-Allow-Origin', b'*'), (b'X-RateLimit-Limit', b'50'), (b'X-RateLimit-Remaining', b'0'), (b'X-RateLimit-Reset', b'1768089600000'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare')])
2026-01-10 23:43:47 | INFO     | httpx | _send_single_request:1025 | HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-10 23:43:47 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.started request=<Request [b'POST']>
2026-01-10 23:43:47 | DEBUG    | httpcore.http11 | trace:47 | receive_response_body.complete
2026-01-10 23:43:47 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:47 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
2026-01-10 23:43:47 | DEBUG    | openai._base_client | request:1016 | HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 10 Jan 2026 18:13:47 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cf-ray': '9bbe26789832f43e-DEL', 'access-control-allow-origin': '*', 'x-ratelimit-limit': '50', 'x-ratelimit-remaining': '0', 'x-ratelimit-reset': '1768089600000', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare'})
2026-01-10 23:43:47 | DEBUG    | openai._base_client | request:1024 | request_id: None
2026-01-10 23:43:47 | DEBUG    | openai._base_client | request:1029 | Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1027, in request
    response.raise_for_status()
  File "/Users/ramlanjekar/Documents/MyProjects/KDAG/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2026-01-10 23:43:47 | DEBUG    | openai._base_client | request:1046 | Re-raising status error
2026-01-10 23:43:47 | WARNING  | extraction_agent.main | extract:79 | Structured output failed: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1768089600000'}, 'provider_name': None}}, 'user_id': 'user_2yUa3CBBFo0pOw0zUiHXS4PLFt0'}. Falling back to JSON parsing
2026-01-10 23:43:47 | DEBUG    | openai._base_client | _build_request:482 | Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-5167dffa-453d-44ba-b2e4-9ff95c558072', 'json_data': {'messages': [{'content': 'You are an extractor agent.\nGenerate 5 queries to retrieve evidence about the character.\n\nBook: The Count of Monte Cristo\nCharacter: Faria\nBackstory: Suspected again in 1815, he was re-arrested and shipped to the Château d’If, this time for life.\n\nReturn only a JSON list of strings, each string being a query.\nExample: ["query 1", "query 2", "query 3"]\n', 'role': 'user'}], 'model': 'nvidia/nemotron-3-nano-30b-a3b:free', 'stream': False}}
2026-01-10 23:43:47 | DEBUG    | openai._base_client | request:978 | Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2026-01-10 23:43:47 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.started request=<Request [b'POST']>
2026-01-10 23:43:47 | DEBUG    | httpcore.http11 | trace:47 | send_request_headers.complete
2026-01-10 23:43:47 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.started request=<Request [b'POST']>
2026-01-10 23:43:47 | DEBUG    | httpcore.http11 | trace:47 | send_request_body.complete
2026-01-10 23:43:47 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.started request=<Request [b'POST']>
2026-01-10 23:43:47 | DEBUG    | httpcore.http11 | trace:47 | receive_response_headers.failed exception=KeyboardInterrupt()
2026-01-10 23:43:47 | DEBUG    | httpcore.http11 | trace:47 | response_closed.started
2026-01-10 23:43:47 | DEBUG    | httpcore.http11 | trace:47 | response_closed.complete
